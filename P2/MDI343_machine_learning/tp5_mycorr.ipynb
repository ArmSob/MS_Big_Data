{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#               Import part\n",
    "###############################################################################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn import neighbors, model_selection\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pylab as pl\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#               Data Generation\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def rand_gauss(n=100, mu=[1, 1], sigmas=[0.1, 0.1]):\n",
    "    \"\"\" Sample n points from a Gaussian variable with center mu,\n",
    "    and std deviation sigma\n",
    "    \"\"\"\n",
    "    d = len(mu)\n",
    "    res = np.random.randn(n, d)\n",
    "    return np.array(res * sigmas + mu)\n",
    "\n",
    "\n",
    "def rand_bi_gauss(n1=100, n2=100, mu1=[1, 1], mu2=[-1, -1], sigmas1=[0.1, 0.1],\n",
    "                  sigmas2=[0.1, 0.1]):\n",
    "    \"\"\" Sample n1 and n2 points from two Gaussian variables centered in mu1,\n",
    "    mu2, with respective std deviations sigma1 and sigma2\n",
    "    \"\"\"\n",
    "    ex1 = rand_gauss(n1, mu1, sigmas1)\n",
    "    ex2 = rand_gauss(n2, mu2, sigmas2)\n",
    "    y = np.hstack([np.ones(n1), -1 * np.ones(n2)])\n",
    "    X = np.vstack([ex1, ex2])\n",
    "    ind = np.random.permutation(n1 + n2)\n",
    "    return X[ind, :], y[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#           Displaying labeled data\n",
    "###############################################################################\n",
    "\n",
    "symlist = ['o', 's', 'D', 'x', '+', '*', 'p', 'v', '-', '^']\n",
    "\n",
    "\n",
    "def plot_2d(data, y=None, w=None, alpha_choice=1):\n",
    "    \"\"\" Plot in 2D the dataset data, colors and symbols according to the\n",
    "    class given by the vector y (if given); the separating hyperplan w can\n",
    "    also be displayed if asked\"\"\"\n",
    "\n",
    "    k = np.unique(y).shape[0]\n",
    "    color_blind_list = sns.color_palette(\"colorblind\", k)\n",
    "    sns.set_palette(color_blind_list)\n",
    "    if y is None:\n",
    "        labs = [\"\"]\n",
    "        idxbyclass = [range(data.shape[0])]\n",
    "    else:\n",
    "        labs = np.unique(y)\n",
    "        idxbyclass = [np.where(y == labs[i])[0] for i in range(len(labs))]\n",
    "\n",
    "    for i in range(len(labs)):\n",
    "        plt.scatter(data[idxbyclass[i], 0], data[idxbyclass[i], 1],\n",
    "                    c=color_blind_list[i], s=80, marker=symlist[i])\n",
    "    plt.ylim([np.min(data[:, 1]), np.max(data[:, 1])])\n",
    "    plt.xlim([np.min(data[:, 0]), np.max(data[:, 0])])\n",
    "    mx = np.min(data[:, 0])\n",
    "    maxx = np.max(data[:, 0])\n",
    "    if w is not None:\n",
    "        plt.plot([mx, maxx], [mx * -w[1] / w[2] - w[0] / w[2],\n",
    "                              maxx * -w[1] / w[2] - w[0] / w[2]],\n",
    "                 \"g\", alpha=alpha_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#           Displaying tools for the Frontiere\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def frontiere(f, X, y, w=None, step=50, alpha_choice=1, colorbar=True,\n",
    "              samples=True):\n",
    "    \"\"\" trace la frontiere pour la fonction de decision f\"\"\"\n",
    "    # construct cmap\n",
    "\n",
    "    min_tot0 = np.min(X[:, 0])\n",
    "    min_tot1 = np.min(X[:, 1])\n",
    "\n",
    "    max_tot0 = np.max(X[:, 0])\n",
    "    max_tot1 = np.max(X[:, 1])\n",
    "    delta0 = (max_tot0 - min_tot0)\n",
    "    delta1 = (max_tot1 - min_tot1)\n",
    "    xx, yy = np.meshgrid(np.arange(min_tot0, max_tot0, delta0 / step),\n",
    "                         np.arange(min_tot1, max_tot1, delta1 / step))\n",
    "    z = np.array([f([vec]) for vec in np.c_[xx.ravel(), yy.ravel()]])\n",
    "    z = z.reshape(xx.shape)\n",
    "    labels = np.unique(z)\n",
    "    color_blind_list = sns.color_palette(\"colorblind\", labels.shape[0])\n",
    "    sns.set_palette(color_blind_list)\n",
    "    my_cmap = ListedColormap(color_blind_list)\n",
    "    plt.imshow(z, origin='lower', interpolation=\"mitchell\", alpha=0.80,\n",
    "               cmap=my_cmap, extent=[min_tot0, max_tot0, min_tot1, max_tot1])\n",
    "    if colorbar is True:\n",
    "        ax = plt.gca()\n",
    "        cbar = plt.colorbar(ticks=labels)\n",
    "        cbar.ax.set_yticklabels(labels)\n",
    "\n",
    "    labels = np.unique(y)\n",
    "    k = np.unique(y).shape[0]\n",
    "    color_blind_list = sns.color_palette(\"colorblind\", k)\n",
    "    sns.set_palette(color_blind_list)\n",
    "    ax = plt.gca()\n",
    "    if samples is True:\n",
    "        for i, label in enumerate(y):\n",
    "            label_num = np.where(labels == label)[0][0]\n",
    "            plt.scatter(X[i, 0], X[i, 1], c=color_blind_list[label_num],\n",
    "                        s=80, marker=symlist[label_num])\n",
    "    plt.xlim([min_tot0, max_tot0])\n",
    "    plt.ylim([min_tot1, max_tot1])\n",
    "    ax.get_yaxis().set_ticks([])\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    if w is not None:\n",
    "        plt.plot([min_tot0, max_tot0],\n",
    "                 [min_tot0 * -w[1] / w[2] - w[0] / w[2],\n",
    "                  max_tot0 * -w[1] / w[2] - w[0] / w[2]],\n",
    "                 \"k\", alpha=alpha_choice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gallery(images, titles, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    pl.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    pl.subplots_adjust(bottom=0, left=.01, right=.99, top=.90,\n",
    "                       hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        pl.subplot(n_row, n_col, i + 1)\n",
    "        pl.imshow(images[i])\n",
    "        pl.title(titles[i], size=12)\n",
    "        pl.xticks(())\n",
    "        pl.yticks(())\n",
    "\n",
    "\n",
    "def title(y_pred, y_test, names):\n",
    "    pred_name = names[int(y_pred)].rsplit(' ', 1)[-1]\n",
    "    true_name = names[int(y_test)].rsplit(' ', 1)[-1]\n",
    "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM et noyaux pour la classification binaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) En vous basant sur la documentation, écrivez un code qui va classifier la classe 1 contre la classe 2 du dataset iris en utilisant les deux premières variables et un noyau linéaire. En laissant la moitié des données de côté, évaluez la performance en généralisation du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and scale data\n",
    "scaler = StandardScaler() \n",
    "iris = datasets.load_iris()\n",
    "X = iris.data \n",
    "X = scaler.fit_transform(X) \n",
    "y = iris.target \n",
    "X = X[y != 0, :2] \n",
    "y = y[y != 0]\n",
    "X_train = X[0::2]\n",
    "X_test = X[1::2]\n",
    "y_train = y[0::2]\n",
    "y_test = y[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le score sur l'échantillon test est de 0.72.\n"
     ]
    }
   ],
   "source": [
    "# estimation sur l'échantillon train\n",
    "svc = SVC(gamma='scale', kernel=\"linear\")\n",
    "svc.fit(X_train, y_train)\n",
    "pred = svc.predict(X_test)\n",
    "# score\n",
    "sc = svc.score(X_test, y_test)\n",
    "print(\"Le score sur l'échantillon test est de \" + str(sc) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Comparez le résultat avec un SVM basé sur noyau polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le score sur l'échantillon test est de 0.6.\n"
     ]
    }
   ],
   "source": [
    "# estimation sur l'échantillon train\n",
    "svc = SVC(gamma='scale', kernel=\"poly\")\n",
    "svc.fit(X_train, y_train)\n",
    "pred = svc.predict(X_test)\n",
    "# score\n",
    "sc = svc.score(X_test, y_test)\n",
    "print(\"Le score sur l'échantillon test est de \" + str(sc) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De façon surprenante, les résultats ne sont pas meilleurs avec un noyau polynomial. Il semblerait que les données ne soient pas bien représentées par ce type de noyau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Montrez que le problème primal résolu par le SVM peut se réécrire sous une forme alternative:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On part du problème initial:\n",
    "\n",
    "$(w^*, w_0^*, \\xi^* \\in R^n) \\in \\underbrace{argmin}_{w \\in H, w_0 \\in R, \\xi \\in R^n} \\left( \\frac{1}{2} ||w^2|| + C \\sum_{i=1}^{n} \\xi_i \\right)$\n",
    "\n",
    "s.c.  $\\xi_i \\geq 0, \\ \\forall i = 1 \\ldots n, \\hspace{10mm} y_i(<w, \\Phi(x_i)> + w_0) \\geq 1 - \\xi_i, \\ \\forall i = 1 \\ldots n$\n",
    "\n",
    "La seconde contrainte implique:\n",
    "\n",
    "$ \\xi_i \\geq 1 - y_i(<w, \\Phi(x_i)> + w_0)$\n",
    "\n",
    "\n",
    "\n",
    "On formule le Lagrangien:\n",
    "\n",
    "$\\mathcal{L}(w, w_0, \\xi, \\alpha, \\mu) = \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} \\xi_i + \\sum_{i=1}^{n} \\alpha_i (1 - y_i (w^T x_i + w_0) - \\xi_i) - \\sum_{i=1}^{n} \\mu_i \\xi_i$\n",
    "\n",
    "On prend les conditions de premier ordre:\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial \\xi_i} = 0 \\Leftrightarrow C - \\alpha_i - \\mu_i = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(trop long de tout réécrire en Latex. On ne fait ici que reprendre le contenu du cours. S'y référer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification de visages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’exemple suivant est un problème de classification de visages. La base de données à utiliser est disponible à l’adresse suivante : https://scikit-learn.org/stable/auto_examples/applications/ plot_face_recognition.html. Vous pouvez choisir deux personnes, par exemple Tony Blair et Colin Powell, pour accélérer le calcul. Aussi, vous pouvez utilisez seulement grayscale (et non le couleur) pour réduire le nombre de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "X = lfw_people.data\n",
    "y = lfw_people.target\n",
    "\n",
    "# retain only data for Powell (label = 1) and Blair (label = 6)\n",
    "X_powell = X[y==1]\n",
    "y_powell = y[y==1]\n",
    "X_blair = X[y==6]\n",
    "y_blair = y[y==6]\n",
    "X = np.concatenate((X_powell, X_blair), axis=0)\n",
    "y = np.concatenate((y_powell, y_blair), axis=0)\n",
    "X_train = X[0::2]\n",
    "X_test = X[1::2]\n",
    "y_train = y[0::2]\n",
    "y_test = y[1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Utilisez les features centrées et réduites. (Pourquoi ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of X_train is [ 1.9763645e-08  1.1105286e-07 -1.2077783e-08 ...  8.1564249e-09\n",
      "  1.1920929e-08  2.0704771e-08]\n",
      "The standard deviation of X_train is [0.99999994 0.99999994 1.         ... 1.0000001  1.         1.        ]\n",
      "0.99999994\n"
     ]
    }
   ],
   "source": [
    "# scale\n",
    "scaler = StandardScaler() \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "print(\"The mean of X_train is \" + str(np.mean(X_train, axis=0)))\n",
    "print(\"The standard deviation of X_train is \" + str(np.std(X_train, axis=0)))\n",
    "print(np.std(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Montrez l’influence du paramètre de régularisation. On pourra par exemple afficher l’erreur de prédiction en fonction de C sur une échelle logarithmique entre 1e5 et 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate grid search\n",
    "parameters = {'C':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]}\n",
    "svc = SVC(gamma='scale', kernel=\"rbf\");\n",
    "clf = GridSearchCV(svc, parameters);\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error with C = 1e-05 is 0.379\n",
      "Prediction error with C = 0.0001 is 0.379\n",
      "Prediction error with C = 0.001 is 0.379\n",
      "Prediction error with C = 0.01 is 0.379\n",
      "Prediction error with C = 0.1 is 0.379\n",
      "Prediction error with C = 1 is 0.168\n",
      "Prediction error with C = 10 is 0.132\n",
      "Prediction error with C = 100 is 0.132\n",
      "Prediction error with C = 1000 is 0.132\n",
      "Prediction error with C = 10000 is 0.132\n",
      "Prediction error with C = 100000 is 0.132\n"
     ]
    }
   ],
   "source": [
    "# collect all prediction errors and display\n",
    "pred_errors = 1 - clf.cv_results_['mean_test_score']\n",
    "for iter, value in enumerate([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]):\n",
    "    print(\"Prediction error with C = \" + str(value) + \" is \" + str(round(pred_errors[iter], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus grandes valeurs de C donnent de meilleurs résultats. Il n'y a toutefois pas de gain significatif pour une valeur de C au-delà de 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le score sur l'échantillon test est de 0.947.\n"
     ]
    }
   ],
   "source": [
    "# estimate model on X_train\n",
    "svc = SVC(gamma='scale', kernel=\"linear\")\n",
    "svc.fit(X_train, y_train)\n",
    "pred = svc.predict(X_test)\n",
    "sc = svc.score(X_test, y_test)\n",
    "print(\"Le score sur l'échantillon test est de \" + str(round(sc, 3)) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) En ajoutant des variables de nuisances (par exemple 300 variables normales centrées réduites), augmentant ainsi le nombre de variables à nombre de points d’apprentissage fixé, montrez que la performance chute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.96756858,  0.79861181,  0.94967103, ...,  0.73617345,\n",
       "         1.48450338,  0.67603846],\n",
       "       [ 0.23965237, -0.27440485,  0.39147991, ...,  0.9585222 ,\n",
       "        -0.25879163, -0.3612656 ],\n",
       "       [-0.89850811, -1.75992424,  3.3648917 , ...,  0.34381152,\n",
       "         0.9269031 ,  0.78103585],\n",
       "       ...,\n",
       "       [ 0.88398359,  0.94289982,  2.54917003, ...,  0.67496613,\n",
       "         0.76590123, -1.80760608],\n",
       "       [-0.73962237, -0.94384347,  1.24710799, ...,  0.6814417 ,\n",
       "         1.31853301,  1.58907436],\n",
       "       [-1.16664067,  1.64751347,  1.47668756, ...,  0.41768046,\n",
       "         2.05485397,  1.1598334 ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create noisy dataset\n",
    "X_size, num_features = X.shape\n",
    "noise = np.random.standard_normal(size=(X_size, 300))\n",
    "X_noisy = np.concatenate((X, noise), axis=1)\n",
    "X_train_noisy = X_noisy[0::2]\n",
    "X_test_noisy = X_noisy[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le score sur l'échantillon test est de 0.953.\n"
     ]
    }
   ],
   "source": [
    "# estimate model on X_train_noisy\n",
    "svc = SVC(gamma='scale', kernel=\"linear\")\n",
    "svc.fit(X_train_noisy, y_train)\n",
    "pred = svc.predict(X_test_noisy)\n",
    "sc = svc.score(X_test_noisy, y_test)\n",
    "print(\"Le score sur l'échantillon test est de \" + str(round(sc, 3)) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Vous pourrez améliorer la prédiction à l’aide d’une réduction de dimension basée sur l’objet sklearn.decomposition.PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA preserving 95% of data variance\n",
    "pca = PCA(n_components = 0.95, svd_solver = 'full')\n",
    "X_pca = pca.fit_transform(X_noisy)\n",
    "X_train_pca = X_pca[0::2]\n",
    "X_test_pca = X_pca[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le score sur l'échantillon test est de 0.937.\n"
     ]
    }
   ],
   "source": [
    "# estimate model on X_train_noisy\n",
    "svc = SVC(gamma='scale', kernel=\"linear\")\n",
    "svc.fit(X_train_pca, y_train)\n",
    "pred = svc.predict(X_test_pca)\n",
    "sc = svc.score(X_test_pca, y_test)\n",
    "print(\"Le score sur l'échantillon test est de \" + str(round(sc, 3)) + \".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
